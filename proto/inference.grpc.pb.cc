// Generated by the gRPC C++ plugin.
// If you make any local change, they will be lost.
// source: inference.proto

#include "inference.pb.h"
#include "inference.grpc.pb.h"

#include <functional>
#include <grpcpp/impl/codegen/async_stream.h>
#include <grpcpp/impl/codegen/async_unary_call.h>
#include <grpcpp/impl/codegen/channel_interface.h>
#include <grpcpp/impl/codegen/client_unary_call.h>
#include <grpcpp/impl/codegen/client_callback.h>
#include <grpcpp/impl/codegen/message_allocator.h>
#include <grpcpp/impl/codegen/method_handler.h>
#include <grpcpp/impl/codegen/rpc_service_method.h>
#include <grpcpp/impl/codegen/server_callback.h>
#include <grpcpp/impl/codegen/server_callback_handlers.h>
#include <grpcpp/impl/codegen/server_context.h>
#include <grpcpp/impl/codegen/service_type.h>
#include <grpcpp/impl/codegen/sync_stream.h>
namespace inference {

static const char* InferenceService_method_names[] = {
  "/inference.InferenceService/PredictBatch",
  "/inference.InferenceService/PredictBatchAsync",
  "/inference.InferenceService/ShutdownServer",
};

std::unique_ptr< InferenceService::Stub> InferenceService::NewStub(const std::shared_ptr< ::grpc::ChannelInterface>& channel, const ::grpc::StubOptions& options) {
  (void)options;
  std::unique_ptr< InferenceService::Stub> stub(new InferenceService::Stub(channel, options));
  return stub;
}

InferenceService::Stub::Stub(const std::shared_ptr< ::grpc::ChannelInterface>& channel, const ::grpc::StubOptions& options)
  : channel_(channel), rpcmethod_PredictBatch_(InferenceService_method_names[0], options.suffix_for_stats(),::grpc::internal::RpcMethod::BIDI_STREAMING, channel)
  , rpcmethod_PredictBatchAsync_(InferenceService_method_names[1], options.suffix_for_stats(),::grpc::internal::RpcMethod::BIDI_STREAMING, channel)
  , rpcmethod_ShutdownServer_(InferenceService_method_names[2], options.suffix_for_stats(),::grpc::internal::RpcMethod::NORMAL_RPC, channel)
  {}

::grpc::ClientReaderWriter< ::inference::Request, ::inference::Response>* InferenceService::Stub::PredictBatchRaw(::grpc::ClientContext* context) {
  return ::grpc::internal::ClientReaderWriterFactory< ::inference::Request, ::inference::Response>::Create(channel_.get(), rpcmethod_PredictBatch_, context);
}

void InferenceService::Stub::async::PredictBatch(::grpc::ClientContext* context, ::grpc::ClientBidiReactor< ::inference::Request,::inference::Response>* reactor) {
  ::grpc::internal::ClientCallbackReaderWriterFactory< ::inference::Request,::inference::Response>::Create(stub_->channel_.get(), stub_->rpcmethod_PredictBatch_, context, reactor);
}

::grpc::ClientAsyncReaderWriter< ::inference::Request, ::inference::Response>* InferenceService::Stub::AsyncPredictBatchRaw(::grpc::ClientContext* context, ::grpc::CompletionQueue* cq, void* tag) {
  return ::grpc::internal::ClientAsyncReaderWriterFactory< ::inference::Request, ::inference::Response>::Create(channel_.get(), cq, rpcmethod_PredictBatch_, context, true, tag);
}

::grpc::ClientAsyncReaderWriter< ::inference::Request, ::inference::Response>* InferenceService::Stub::PrepareAsyncPredictBatchRaw(::grpc::ClientContext* context, ::grpc::CompletionQueue* cq) {
  return ::grpc::internal::ClientAsyncReaderWriterFactory< ::inference::Request, ::inference::Response>::Create(channel_.get(), cq, rpcmethod_PredictBatch_, context, false, nullptr);
}

::grpc::ClientReaderWriter< ::inference::Request, ::inference::Response>* InferenceService::Stub::PredictBatchAsyncRaw(::grpc::ClientContext* context) {
  return ::grpc::internal::ClientReaderWriterFactory< ::inference::Request, ::inference::Response>::Create(channel_.get(), rpcmethod_PredictBatchAsync_, context);
}

void InferenceService::Stub::async::PredictBatchAsync(::grpc::ClientContext* context, ::grpc::ClientBidiReactor< ::inference::Request,::inference::Response>* reactor) {
  ::grpc::internal::ClientCallbackReaderWriterFactory< ::inference::Request,::inference::Response>::Create(stub_->channel_.get(), stub_->rpcmethod_PredictBatchAsync_, context, reactor);
}

::grpc::ClientAsyncReaderWriter< ::inference::Request, ::inference::Response>* InferenceService::Stub::AsyncPredictBatchAsyncRaw(::grpc::ClientContext* context, ::grpc::CompletionQueue* cq, void* tag) {
  return ::grpc::internal::ClientAsyncReaderWriterFactory< ::inference::Request, ::inference::Response>::Create(channel_.get(), cq, rpcmethod_PredictBatchAsync_, context, true, tag);
}

::grpc::ClientAsyncReaderWriter< ::inference::Request, ::inference::Response>* InferenceService::Stub::PrepareAsyncPredictBatchAsyncRaw(::grpc::ClientContext* context, ::grpc::CompletionQueue* cq) {
  return ::grpc::internal::ClientAsyncReaderWriterFactory< ::inference::Request, ::inference::Response>::Create(channel_.get(), cq, rpcmethod_PredictBatchAsync_, context, false, nullptr);
}

::grpc::Status InferenceService::Stub::ShutdownServer(::grpc::ClientContext* context, const ::inference::Empty& request, ::inference::Empty* response) {
  return ::grpc::internal::BlockingUnaryCall< ::inference::Empty, ::inference::Empty, ::grpc::protobuf::MessageLite, ::grpc::protobuf::MessageLite>(channel_.get(), rpcmethod_ShutdownServer_, context, request, response);
}

void InferenceService::Stub::async::ShutdownServer(::grpc::ClientContext* context, const ::inference::Empty* request, ::inference::Empty* response, std::function<void(::grpc::Status)> f) {
  ::grpc::internal::CallbackUnaryCall< ::inference::Empty, ::inference::Empty, ::grpc::protobuf::MessageLite, ::grpc::protobuf::MessageLite>(stub_->channel_.get(), stub_->rpcmethod_ShutdownServer_, context, request, response, std::move(f));
}

void InferenceService::Stub::async::ShutdownServer(::grpc::ClientContext* context, const ::inference::Empty* request, ::inference::Empty* response, ::grpc::ClientUnaryReactor* reactor) {
  ::grpc::internal::ClientCallbackUnaryFactory::Create< ::grpc::protobuf::MessageLite, ::grpc::protobuf::MessageLite>(stub_->channel_.get(), stub_->rpcmethod_ShutdownServer_, context, request, response, reactor);
}

::grpc::ClientAsyncResponseReader< ::inference::Empty>* InferenceService::Stub::PrepareAsyncShutdownServerRaw(::grpc::ClientContext* context, const ::inference::Empty& request, ::grpc::CompletionQueue* cq) {
  return ::grpc::internal::ClientAsyncResponseReaderHelper::Create< ::inference::Empty, ::inference::Empty, ::grpc::protobuf::MessageLite, ::grpc::protobuf::MessageLite>(channel_.get(), cq, rpcmethod_ShutdownServer_, context, request);
}

::grpc::ClientAsyncResponseReader< ::inference::Empty>* InferenceService::Stub::AsyncShutdownServerRaw(::grpc::ClientContext* context, const ::inference::Empty& request, ::grpc::CompletionQueue* cq) {
  auto* result =
    this->PrepareAsyncShutdownServerRaw(context, request, cq);
  result->StartCall();
  return result;
}

InferenceService::Service::Service() {
  AddMethod(new ::grpc::internal::RpcServiceMethod(
      InferenceService_method_names[0],
      ::grpc::internal::RpcMethod::BIDI_STREAMING,
      new ::grpc::internal::BidiStreamingHandler< InferenceService::Service, ::inference::Request, ::inference::Response>(
          [](InferenceService::Service* service,
             ::grpc::ServerContext* ctx,
             ::grpc::ServerReaderWriter<::inference::Response,
             ::inference::Request>* stream) {
               return service->PredictBatch(ctx, stream);
             }, this)));
  AddMethod(new ::grpc::internal::RpcServiceMethod(
      InferenceService_method_names[1],
      ::grpc::internal::RpcMethod::BIDI_STREAMING,
      new ::grpc::internal::BidiStreamingHandler< InferenceService::Service, ::inference::Request, ::inference::Response>(
          [](InferenceService::Service* service,
             ::grpc::ServerContext* ctx,
             ::grpc::ServerReaderWriter<::inference::Response,
             ::inference::Request>* stream) {
               return service->PredictBatchAsync(ctx, stream);
             }, this)));
  AddMethod(new ::grpc::internal::RpcServiceMethod(
      InferenceService_method_names[2],
      ::grpc::internal::RpcMethod::NORMAL_RPC,
      new ::grpc::internal::RpcMethodHandler< InferenceService::Service, ::inference::Empty, ::inference::Empty, ::grpc::protobuf::MessageLite, ::grpc::protobuf::MessageLite>(
          [](InferenceService::Service* service,
             ::grpc::ServerContext* ctx,
             const ::inference::Empty* req,
             ::inference::Empty* resp) {
               return service->ShutdownServer(ctx, req, resp);
             }, this)));
}

InferenceService::Service::~Service() {
}

::grpc::Status InferenceService::Service::PredictBatch(::grpc::ServerContext* context, ::grpc::ServerReaderWriter< ::inference::Response, ::inference::Request>* stream) {
  (void) context;
  (void) stream;
  return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
}

::grpc::Status InferenceService::Service::PredictBatchAsync(::grpc::ServerContext* context, ::grpc::ServerReaderWriter< ::inference::Response, ::inference::Request>* stream) {
  (void) context;
  (void) stream;
  return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
}

::grpc::Status InferenceService::Service::ShutdownServer(::grpc::ServerContext* context, const ::inference::Empty* request, ::inference::Empty* response) {
  (void) context;
  (void) request;
  (void) response;
  return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
}


}  // namespace inference

